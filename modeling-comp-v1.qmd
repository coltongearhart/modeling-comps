---
title: "Modeling Competition V1"
format:
  html:
    toc: true
    toc-location: left
    df-print: kable
execute: 
  warning: false
  cache: true
---

## Setup

```{r}
#| label: setup

# load packages
library(tidyverse)
library(tidymodels)
library(modeldata)
library(glmnet)

```

## View dataset

```{r}
#| label: data

# set seed with Mark
set.seed(1)

# make test / train split
ames_split <- initial_split(data = ames %>% mutate(row_number = row_number()),
                            prop = .8)
nrow(ames_split %>% testing())

# save train data
ames_train <- ames_split %>% training() %>% janitor::clean_names()
glimpse(ames_train)

# check to make sure test matches marks
ames_train %>% pull(row_number) %>% head()

# remove row number
ames_train %<>% select(-row_number)

```

## Bootstrap models

Steps

- Loop $b$ times:

    1. Create bootstrap sample of train data

    2. Fit 10 fold CV to find optimal $\lambda$ for `sales_price ~ only numeric predictors` (think `cv.glmnet()` only works with numeric predictors or there was too many levels of the other??).

    3. Fit lasso regression using optimal $\lambda$.

    4. Keep track of variables not shrunk to zero.

```{r}
#| label: bootstrap models

# loop to do variable selection from b bootstrap samples
# -> combine results from all samples
b <- 10000
results <- parallel::mclapply(X = 1:b, FUN = function(X) {
  
  # create bootstrap sample
  ames_boot = ames_train[sample(x = 1:nrow(ames_train), size = nrow(ames_train), replace = TRUE), ]
  
  # find optimal lambda from 10-fold cross validation
  x = ames_boot %>% select(where(is.numeric), -sale_price) %>% as.matrix
  y = ames_boot %>% select(sale_price) %>% as.matrix
  lambda = cv.glmnet(x =  x, y = y)$lambda.min
  
  # fit lasso model based on optimal lambda
  mod_boot = glmnet(x = x, y = y, lambda = lambda)
  
  # get variable names
  betas = mod_boot$beta
  
  # get nonzero beta coefficients
  # -> convert to dataframe in order determine if variable should be kept (have to convert to matrix first)
  # -> transpose so can row bind final results and summarize with column sums for frequency kept
  vars = betas %>% 
    as.matrix %>% 
    data.frame %>% 
    mutate(kept = if_else(s0 != 0, 1, 0)) %>% 
    select(-s0) %>% 
    t
  
  # status counter
  #if (X %% 10 == 0) print(X)
  
  return(vars)
  
}, mc.cores = 4) %>% 
  Reduce(x = ., f = rbind)
rownames(results) <- NULL

dim(results)
head(results) %>% data.frame

```

## Variable selection

Steps

1. Calculate the total number of samples (out of $b$) that each variable was kept.

2. Create variable selection plot, ordered from least to most (in terms of proportion of samples kept) with a threshold plotted.

3. Only keep variables that we kept above the threshold.

4. Convert final model variables into to formula

```{r}
#| label: variable-selection

# summarize results for number of times variable was kept in model
results_summ <- results %>% 
  as.data.frame() %>% 
  summarize(across(everything(), sum))
results_summ

# create variable selection plot
# set threshold
th <- 0.8
results_summ %>% 
  t %>% 
  as.data.frame %>% 
  rownames_to_column(var = "var") %>% 
  rename(kept = V1) %>% 
  arrange(kept) %>% 
  mutate(var = fct_inorder(f = var)) %>% 
  ggplot(aes(x = kept / b,
             y = var)) + 
  geom_point() + 
  geom_vline(aes(xintercept = th),
             color = "blue") + 
  labs(title = "Variable selection plot",
       x = "Proportion of bootstrap samples coefficient not shrunk to 0",
       y = "Variable") + 
  theme_bw()

# select final variables
final_vars <- results_summ %>% 
  pivot_longer(cols = everything(),
               names_to = "var",
               values_to = "kept_n") %>% 
  mutate(kept_prop = kept_n / b) %>% 
  filter(kept_prop > th)
nrow(final_vars)

# create formula
formula_final <- paste("sale_price",
                   paste(final_vars$var, collapse = " + "),
                   sep = " ~ ") %>% as.formula
formula_final

# hard coded formula
formula_final <- c("sale_price ~ lot_frontage + lot_area + year_built + year_remod_add + 
    mas_vnr_area + bsmt_unf_sf + total_bsmt_sf + first_flr_sf + 
    gr_liv_area + bsmt_full_bath + bedroom_abv_gr + kitchen_abv_gr + 
    tot_rms_abv_grd + fireplaces + garage_cars + garage_area + 
    wood_deck_sf + screen_porch + pool_area + misc_val + latitude") %>% as.formula

# fit final model
mod_final <- lm(formula_final, data = ames_train)

```

## Test

```{r}
#| label: testing

# save test data
ames_test <- ames_split %>% testing() %>% janitor::clean_names()

# calculate predictions for test set
preds <- predict(mod_final, newdata = ames_test)

# calculate rmse
bind_cols(ames_test, preds = preds) %>% yardstick::rmse(sale_price, preds)

```

## Incorporate model diagnostics

```{r}
#| label: diagnostics

# check final model diagnostics
plot(mod_final, which = 1:2)

```

Seems like increasing variance and non-normal (heavy-tailed) errors. So try to remedy.

```{r}
#| label: boxcox

# run boxcox procedure
# -> plot MLE of lambda
MASS::boxcox(mod_final)

# extract lambda
bc <- MASS::boxcox(mod_final)
bc$x[which.max(bc$y)]

```

## Transform model

Use $Y' = \ln(Y)$. Now fit transformed model.

```{r}
#| label: transformed-model

# hard coded formula
formula_final_prime <- c("log(sale_price) ~ lot_frontage + lot_area + year_built + year_remod_add + 
    mas_vnr_area + bsmt_unf_sf + total_bsmt_sf + first_flr_sf + 
    gr_liv_area + bsmt_full_bath + bedroom_abv_gr + kitchen_abv_gr + 
    tot_rms_abv_grd + fireplaces + garage_cars + garage_area + 
    wood_deck_sf + screen_porch + pool_area + misc_val + latitude") %>% as.formula

# fit transformed model
mod_final_prime <- lm(formula_final_prime, data = ames_train, x = TRUE)

```

Now inspect transformed model diagnostics.

```{r}
#| label: diagnostics-transformed

# check final model diagnostics
plot(mod_final_prime, which = 1:2)

```

See improvement in non-constant variance and slightly so in the normality of the errors. Also see increase in $R^2_{adj}$.

```{r}
#| label: summaries

# compare model summaries
map(list(mod_final, mod_final_prime), summary)

```

## Test with backtransformed predictions

With $Y' = \ln(Y)$ $\Longrightarrow$ $Y = f'(Y') = e^{ln(Y)} = e^{Y'}$. Now back transform predictions and test.

```{r}
#| label: testing-transformed

# calculate predictions for test set
preds_prime <- predict(mod_final_prime, newdata = ames_test)

# back transform predictions and calculate rmse
bind_cols(ames_test, preds = exp(preds_prime)) %>% yardstick::rmse(sale_price, preds)

```

## Check for higher order terms

```{r}
#| label: residual-plots-x
#| eval: false

# get residuals
e <- residuals(mod_final_prime)

# plot against each X
mod_final_prime$x %>% 
  data.frame %>% 
  select(-1) %>% 
  lapply(function(x) {
    plot(x = x, y = e)
    lines(lowess(x = x, y = e), col = "red") %>% return
 })

# no signs of curvature

```