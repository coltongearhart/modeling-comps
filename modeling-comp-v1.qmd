---
title: "Modeling Competition V1"
format:
  html:
    toc: true
    toc-location: left
    df-print: kable
execute: 
  warning: false
  cache: true
---

## Setup

```{r}

# load packages
library(magrittr)
library(tidyverse)
library(tidymodels)
library(furrr)
library(modeldata)
library(glmnet)

```

### View dataset

```{r}

# set seed with Mark
set.seed(1)

# make test / train split
split_ames <- initial_split(data = ames %>% mutate(row_number = row_number()),
                            prop = .8)
nrow(split_ames %>% testing())

# save train data
data_train <- split_ames %>% training() %>% janitor::clean_names()
glimpse(data_train)

# check to make sure test matches marks
data_train %>% pull(row_number) %>% head()

# remove row number
data_train %<>% select(-row_number)

# save test data
data_test <- split_ames %>% testing() %>% janitor::clean_names()

```

## Bootstrap models

### Fit models and do variable selection

Steps

- Loop $b$ times:

    1. Create bootstrap sample of train data

    2. Fit 10 fold CV to find optimal $\lambda$ for `sales_price ~ only numeric predictors` (think `cv.glmnet()` only works with numeric predictors or there was too many levels of the other??).

    3. Fit lasso regression using optimal $\lambda$.

    4. Keep track of variables not shrunk to zero.

Create bootstrap samples.

```{r}

# create bootstrap samples
b <- 1000
data_boot <- rsample::bootstraps(data_train, times = b)
tmp_boot <- data_boot$splits[[1]] %>% analysis
head(tmp_boot)

```

Define helper functions for fitting models.

```{r}

# define function to fit lasso based on optimal lambda
# -> 1) setup modeling frame
# --> creates design matrix of only numeric predictors and creates response vector
# -> 2) find optimal lambda
# --> run 10-fold cross validation and extract best lambda
# -> 3) fit lasso model
fit_lasso <- function(df, y) {
  
  # create design matrix
  X = df %>% 
    select(where(is.numeric), -any_of(y)) %>% 
    as.matrix
  
  # create response vector
  y = df %>% select(any_of(y)) %>% as.matrix
  
  # find lambda
  lambda = cv.glmnet(x = X, y = y)$lambda.min
  
  # fit lasso model based on optimal lambda
  mod_boot = glmnet(x = X, y = y, lambda = lambda)
  
  return(mod_boot)
}

# test function
tmp_mod <- tmp_boot %>% fit_lasso(y = "sale_price")
tmp_mod

# define function to report kept variables in the model
report_vars <- function(mod) {
  
   # get variable names
  betas = mod$beta
  
  # get nonzero beta coefficients
  # -> convert to dataframe in order determine if variable should be kept (have to convert to matrix first)
  # -> transpose so can row bind final results and summarize with column sums for frequency kept
  vars = betas %>% 
    as.matrix %>% 
    data.frame %>% 
    mutate(kept = if_else(s0 != 0, 1, 0)) %>% 
    select(-s0) %>% 
    t %>% 
    data.frame
  
  rownames(vars) = NULL
  
  return(vars)
}

# test function
tmp_vars <- tmp_mod %>% report_vars
glimpse(tmp_vars)

```

Fit all bootstrap models and get results.

Steps

1. Calculate the total number of samples (out of $b$) that each variable was kept.

2. Create variable selection plot, ordered from least to most (in terms of proportion of samples kept) with a threshold plotted.

3. Only keep variables that we kept above the threshold.

4. Convert final model variables into to formula

```{r}

# setup plan
plan(multisession, workers = availableCores()-4)

# fit all bootstrap models
data_boot$models <- data_boot$splits %>% future_map(function(split) {
  analysis(split) %>% fit_lasso(y = "sale_price")
}, .progress = TRUE)

# get kept variables
data_boot$variables <- data_boot$models %>% map(\(mod) report_vars(mod))

# summarize results for number of times variable was kept in model
results_boot <- data_boot$variables %>% 
  reduce(bind_rows) %>% 
  summarize(across(everything(), sum)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "kept_n") %>% 
  mutate(kept_prop = kept_n / b) %>% 
  arrange(desc(kept_n))

# create variable selection plot
# set threshold
th <- 0.8
results_boot %>% 
  mutate(variable = fct_rev(fct_inorder(f = variable))) %>% 
  ggplot(aes(x = kept_prop,
             y = variable)) + 
  geom_point() + 
  geom_vline(aes(xintercept = th),
             color = "blue") + 
  labs(title = "Variable selection plot",
       x = "Proportion of bootstrap samples coefficient not shrunk to 0",
       y = "Variable") + 
  theme_bw()

# select final variables
vars_boot <- results_boot %>% 
  filter(kept_prop > th)
nrow(vars_boot)

```

```{r}

# create formula
formula_boot <- paste("sale_price",
                   paste(vars_boot$variable, collapse = " + "),
                   sep = " ~ ") %>% as.formula
formula_boot

# hard coded formula
# -> note this formula is from when run on b = 10000
formula_boot <- "sale_price ~ year_built + year_remod_add + mas_vnr_area + kitchen_abv_gr + latitude + total_bsmt_sf + fireplaces + garage_cars + screen_porch + wood_deck_sf + bsmt_full_bath + bedroom_abv_gr + gr_liv_area + lot_frontage + tot_rms_abv_grd + garage_area + lot_area + misc_val + pool_area + bsmt_unf_sf + first_flr_sf" %>% as.formula

# specify model
spec_boot <- linear_reg()

# fit the model based on bootstrap variable selection
fit_boot <- spec_boot %>% 
  fit(formula_boot, data = data_train)

```

### Test

```{r}

# calculate rmse
augment(fit_boot, new_data = data_test) %>% 
  rmse(truth = sale_price, estimate = .pred)

```

## Incorporate model diagnostics

```{r}

# check final model diagnostics
plot(fit_boot$fit, which = 1:2)

```

Seems like increasing variance and non-normal (heavy-tailed) errors. So try to remedy.

```{r}

# run boxcox procedure
# -> plot MLE of lambda
tmp_mod <- lm(formula(fit_boot$fit), data_train)
MASS::boxcox(tmp_mod)

# extract lambda
bc <- MASS::boxcox(tmp_mod)
bc$x[which.max(bc$y)]

```

### Transform model

Use $Y' = \ln(Y)$. Now fit transformed model.

```{r}

# hard coded formula
formula_boot_prime <- c("log(sale_price) ~ year_built + year_remod_add + mas_vnr_area + kitchen_abv_gr + latitude + total_bsmt_sf + fireplaces + garage_cars + screen_porch + wood_deck_sf + bsmt_full_bath + bedroom_abv_gr + gr_liv_area + lot_frontage + tot_rms_abv_grd + garage_area + lot_area + misc_val + pool_area + bsmt_unf_sf + first_flr_sf") %>% as.formula

# fit transformed model model based on bootstrap variable selection
fit_boot_prime <- spec_boot %>% 
  fit(formula_boot_prime, data = data_train)

```

Now inspect transformed model diagnostics.

```{r}

# check final model diagnostics
plot(fit_boot_prime$fit, which = 1:2)

```

See improvement in non-constant variance and slightly so in the normality of the errors. Also see increase in $R^2_{adj}$.

```{r}

# compare model summaries
map(list(fit_boot, fit_boot_prime), glance)

```

### Test with backtransformed predictions

With $Y' = \ln(Y)$ $\Longrightarrow$ $Y = f'(Y') = e^{ln(Y)} = e^{Y'}$. Now back transform predictions and test.

```{r}

# calculate predictions for test set
preds_prime <- predict(fit_boot_prime, new_data = data_test)

# back transform predictions and calculate rmse
rmse_vec(truth = data_test$sale_price %>% as.numeric,
         estimate = exp(preds_prime)$.pred)

```

## Check for higher order terms

Plotting residuals against each $X$ to check for signs of curvature.

```{r}
#| eval: false

# get residuals
e <- residuals(fit_boot_prime$fit) 

# plot against each X
nms_x <- colnames(fit_boot_prime$fit$model[, -1])
map2(data.frame(fit_boot_prime$fit$model)[, -1], nms_x, function(x, nm) {
  plot(x = x, y = e, main = nm)
  lines(lowess(x = x, y = e), col = "red")
  abline(h = 0, col = "grey")
})

# no signs of curvature

```

### Check for interaction terms

Plotting residuals against each $X_i X_j$ to check for signs of interaction.

```{r}

# hard coded formula with all terms crossed
formula_boot_prime_crossed <- c("log(sale_price) ~ (year_built + year_remod_add + mas_vnr_area + kitchen_abv_gr + latitude + total_bsmt_sf + fireplaces + garage_cars + screen_porch + wood_deck_sf + bsmt_full_bath + bedroom_abv_gr + gr_liv_area + lot_frontage + tot_rms_abv_grd + garage_area + lot_area + misc_val + pool_area + bsmt_unf_sf + first_flr_sf)^2") %>% as.formula

# get residuals of original model
e <- residuals(fit_boot_prime$fit)

# fit model with all possible interaction terms
# -> using to extract design matrix of interaction terms
# -> NOTE: using lm() cause need design matrix
mod_boot_prime_crossed <- lm(formula_boot_prime_crossed,data = data_train, x = TRUE)
terms <- tidy(mod_boot_prime_crossed)

```

```{r}
#| eval: false

# extract interaction terms
# -> interaction symbol : gets recoded as .
X_int <- mod_boot_prime_crossed$x %>% 
  data.frame %>% 
  select(contains("."), -contains("Intercept"))

# plot residuals against all interaction terms
nms_x_int <- colnames(X_int)
map2(X_int, nms_x_int, function(x, nm) {
  plot(x = x, y = e, main = nm)
  lines(lowess(x = x, y = e), col = "red")
  abline(h = 0, col = "grey")
})

# no obvious visual signs of significant interactions

# partial F test on ALL interaction terms
anova(mod_boot_prime_crossed, fit_boot_prime$fit)

```

This shows at least one interaction term is significant. So lets find out which ones.

```{r}

# get only "significant" interaction terms
# -> essentially a z value
t_crit <- qt(p = 0.975, df = nrow(data_train) - 1)
terms %>% filter(abs(statistic) > t_crit) %>% 
  filter(str_detect(term, ":")) %>% 
  pull(term) %>% 
  reduce(paste0, " + ")

# copy and paste to hardcoded formula from results above
formula_boot_prime_int <- "log(sale_price) ~ year_built + year_remod_add + mas_vnr_area + kitchen_abv_gr + latitude + total_bsmt_sf + fireplaces + garage_cars + screen_porch + wood_deck_sf + bsmt_full_bath + bedroom_abv_gr + gr_liv_area + lot_frontage + tot_rms_abv_grd + garage_area + lot_area + misc_val + pool_area + bsmt_unf_sf + first_flr_sf + year_built:latitude + year_built:fireplaces + year_built:garage_cars + year_built:gr_liv_area + year_built:garage_area + year_built:first_flr_sf + year_remod_add:latitude + year_remod_add:bedroom_abv_gr + year_remod_add:gr_liv_area + year_remod_add:first_flr_sf + mas_vnr_area:bedroom_abv_gr + mas_vnr_area:tot_rms_abv_grd + mas_vnr_area:garage_area + mas_vnr_area:lot_area + kitchen_abv_gr:latitude + kitchen_abv_gr:tot_rms_abv_grd + latitude:fireplaces + total_bsmt_sf:gr_liv_area + fireplaces:garage_cars + fireplaces:bedroom_abv_gr + fireplaces:gr_liv_area + fireplaces:garage_area + garage_cars:gr_liv_area + wood_deck_sf:tot_rms_abv_grd + bedroom_abv_gr:bsmt_unf_sf + gr_liv_area:garage_area + gr_liv_area:lot_area + lot_frontage:misc_val + garage_area:lot_area + garage_area:bsmt_unf_sf + lot_area:misc_val" %>% as.formula

# fit transformed model model based on bootstrap variable selection
fit_boot_prime_int <- spec_boot %>% 
  fit(formula_boot_prime_int, data = data_train)

# more partial F tests on interaction terms
# -> first to see if selected interaction model is better than additive model
# -> then to see if selected interaction model is better than all interaction model
anova(fit_boot_prime_int$fit, fit_boot_prime$fit)
anova(mod_boot_prime_crossed, fit_boot_prime_int$fit)

# definite yes to first test
# marginal yes to second test

# check R2s for the two interaction models
# compare model summaries
map(list(mod_boot_prime_crossed, fit_boot_prime_int$fit), glance)

# not much increase in model fit, so going with the more parsimonious model

```

### Test with backtransformed predictions

```{r}

# calculate predictions for test set
preds_prime_int <- predict(fit_boot_prime_int, new_data = data_test)

# back transform predictions and calculate rmse
rmse_vec(truth = data_test$sale_price %>% as.numeric,
         estimate = exp(preds_prime_int)$.pred)

```

BEST MODEL!!!